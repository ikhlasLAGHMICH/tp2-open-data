#!/usr/bin/env python3
"""Script principal du pipeline."""
import argparse
from datetime import datetime
from pathlib import Path
import pandas as pd
import sys

# Import des modules internes
from .fetchers.openfoodfacts import OpenFoodFactsFetcher
from .enricher import DataEnricher
from .transformer import DataTransformer
from .quality import QualityAnalyzer
from .storage import save_raw_json, save_parquet
from .config import MAX_ITEMS, PROCESSED_DIR
from .logger import setup_logger

# Initialisation du logger
logger = setup_logger("Orchestrator")

def get_existing_ids(category: str) -> set:
    """
    RÃ©cupÃ¨re les IDs (codes barres) des produits dÃ©jÃ  stockÃ©s en Parquet.
    Permet d'Ã©viter de traiter deux fois le mÃªme produit.
    """
    # On cherche tous les fichiers parquet correspondant Ã  la catÃ©gorie
    files = list(PROCESSED_DIR.glob(f"{category}_*.parquet"))
    
    existing_ids = set()
    logger.info(f"ğŸ”„ VÃ©rification de l'historique dans {len(files)} fichiers...")

    for f in files:
        try:
            # On lit uniquement la colonne 'code' pour aller trÃ¨s vite
            df = pd.read_parquet(f, columns=['code'])
            # On ajoute les codes Ã  notre set (ensemble unique)
            existing_ids.update(df['code'].astype(str).tolist())
        except Exception as e:
            logger.warning(f"Impossible de lire {f.name}: {e}")
            continue
            
    if existing_ids:
        logger.info(f"â„¹ï¸ {len(existing_ids)} produits dÃ©jÃ  existants en base.")
    return existing_ids

def run_pipeline(
    category: str,
    max_items: int = MAX_ITEMS,
    skip_enrichment: bool = False,
    incremental: bool = False,  # <--- NOUVEAU PARAMÃˆTRE
    verbose: bool = True
) -> dict:
    """
    ExÃ©cute le pipeline complet.
    """
    stats = {"start_time": datetime.now()}
    
    logger.info("=" * 60)
    logger.info(f"ğŸš€ PIPELINE OPEN DATA - CatÃ©gorie : {category.upper()}")
    if incremental:
        logger.info("ğŸ”„ Mode IncrÃ©mental : ACTIVÃ‰")
    logger.info("=" * 60)
    
    # === Ã‰TAPE 0 : Chargement de l'historique (Si incrÃ©mental) ===
    existing_ids = set()
    if incremental:
        existing_ids = get_existing_ids(category)

    # === Ã‰TAPE 1 : Acquisition ===
    logger.info("ğŸ“¥ Ã‰TAPE 1 : Acquisition des donnÃ©es (OpenFoodFacts)")
    fetcher = OpenFoodFactsFetcher()
    
    # On rÃ©cupÃ¨re les donnÃ©es brutes
    # Note : Fetcher ne filtre pas en amont (l'API OFF ne le permet pas facilement par ID)
    raw_products = list(fetcher.fetch_all(category, max_items, verbose))
    
    if not raw_products:
        logger.error("âŒ Aucun produit rÃ©cupÃ©rÃ©.")
        return {"error": "No data fetched"}
    
    # --- FILTRAGE INCRÃ‰MENTAL ---
    products = raw_products
    if incremental and existing_ids:
        # On ne garde que les produits dont le code n'est PAS dans l'historique
        products = [p for p in raw_products if str(p.get('code')) not in existing_ids]
        
        skipped_count = len(raw_products) - len(products)
        if skipped_count > 0:
            logger.info(f"â© {skipped_count} produits dÃ©jÃ  connus ignorÃ©s.")
        
        if not products:
            logger.info("âœ… Aucun NOUVEAU produit Ã  traiter. Pipeline terminÃ©.")
            return {"status": "skipped_no_new_data"}
    # -----------------------------
    
    # Sauvegarde de sÃ©curitÃ©
    raw_path = save_raw_json(products, f"{category}_raw")
    logger.info(f"ğŸ’¾ Sauvegarde brute ({len(products)} items) : {raw_path.name}")
    stats["fetcher"] = fetcher.get_stats()
    
    # === Ã‰TAPE 2 : Enrichissement ===
    if not skip_enrichment:
        logger.info("ğŸŒ Ã‰TAPE 2 : Enrichissement (GÃ©ocodage API Adresse)")
        enricher = DataEnricher()
        
        addresses = enricher.extract_addresses(products, "stores")
        
        if addresses:
            limit_geo = 100
            logger.info(f"   GÃ©ocodage des {min(len(addresses), limit_geo)} premiÃ¨res adresses uniques...")
            geo_cache = enricher.build_geocoding_cache(addresses[:limit_geo])
            
            products = enricher.enrich_products(products, geo_cache, "stores")
            stats["enricher"] = enricher.get_stats()
        else:
            logger.warning("âš ï¸ Pas d'adresses trouvÃ©es dans le champ 'stores'.")
    else:
        logger.info("â­ï¸ Ã‰TAPE 2 : Enrichissement ignorÃ©")
    
    # === Ã‰TAPE 3 : Transformation ===
    logger.info("ğŸ”§ Ã‰TAPE 3 : Transformation et nettoyage")
    df = pd.DataFrame(products)
    
    transformer = DataTransformer(df)
    df_clean = (
        transformer
        .remove_duplicates(subset=['code'])
        .handle_missing_values(numeric_strategy='median', text_strategy='unknown')
        .normalize_text_columns(['brands', 'categories', 'stores'])
        .add_derived_columns()
        .get_result()
    )
    
    logger.info(f"   Transformations appliquÃ©es : {len(transformer.transformations_applied)}")
    stats["transformer"] = {"transformations": transformer.transformations_applied}
    
    # === Ã‰TAPE 4 : QualitÃ© ===
    logger.info("ğŸ“Š Ã‰TAPE 4 : Analyse de qualitÃ©")
    analyzer = QualityAnalyzer(df_clean)
    metrics = analyzer.analyze()
    
    logger.info(f"   ğŸ“ Note globale : {metrics.quality_grade}")
    logger.info(f"   âœ… ComplÃ©tude : {metrics.completeness_score * 100:.1f}%")
    
    report_path = analyzer.generate_report(f"{category}_quality")
    stats["quality"] = metrics.model_dump()
    
    # === Ã‰TAPE 5 : Stockage ===
    logger.info("ğŸ’¾ Ã‰TAPE 5 : Stockage final (Parquet)")
    output_path = save_parquet(df_clean, category)
    stats["output_path"] = str(output_path)
    
    # === FIN ===
    stats["end_time"] = datetime.now()
    stats["duration_seconds"] = (stats["end_time"] - stats["start_time"]).seconds
    
    logger.info("=" * 60)
    logger.info("âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS")
    logger.info("=" * 60)
    logger.info(f"â±ï¸  DurÃ©e : {stats['duration_seconds']} secondes")
    logger.info(f"ğŸ“¦ Nouveaux produits : {len(df_clean)}")
    logger.info(f"ğŸ“‚ Fichier final : {output_path}")
    
    return stats

def main():
    """Point d'entrÃ©e CLI."""
    parser = argparse.ArgumentParser(description="Pipeline Open Data TP2")
    parser.add_argument("--category", "-c", default="chocolats", help="CatÃ©gorie")
    parser.add_argument("--max-items", "-m", type=int, default=50, help="Nombre max")
    parser.add_argument("--skip-enrichment", "-s", action="store_true", help="Sauter gÃ©ocodage")
    parser.add_argument("--incremental", "-i", action="store_true", help="Ne traiter que les nouveaux produits")
    parser.add_argument("--verbose", "-v", action="store_true", default=True)
    
    args = parser.parse_args()
    
    try:
        run_pipeline(
            category=args.category,
            max_items=args.max_items,
            skip_enrichment=args.skip_enrichment,
            incremental=args.incremental, # <--- Passage de l'argument
            verbose=args.verbose
        )
    except KeyboardInterrupt:
        logger.warning("ğŸ›‘ Pipeline arrÃªtÃ© par l'utilisateur.")
        sys.exit(0)
    except Exception as e:
        logger.critical(f"âŒ Erreur critique : {e}", exc_info=True)
        raise e

if __name__ == "__main__":
    main()